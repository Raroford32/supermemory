/**
 * OpenRouter API Client for Exploit Research Memory System
 *
 * Embedding Model: qwen/qwen3-embedding-8b (4096 dimensions)
 * LLM Model: openai/gpt-5.1
 */

// ============================================================================
// CONFIGURATION
// ============================================================================

export const OPENROUTER_CONFIG = {
  baseUrl: "https://openrouter.ai/api/v1",
  embeddingModel: "qwen/qwen3-embedding-8b",
  embeddingDimensions: 4096,
  llmModel: "openai/gpt-5.1",
  defaultMaxTokens: 4096,
  defaultTemperature: 0.7,
} as const

export interface OpenRouterConfig {
  apiKey: string
  baseUrl?: string
  embeddingModel?: string
  llmModel?: string
  siteUrl?: string
  siteName?: string
  verbose?: boolean
}

export interface EmbeddingRequest {
  input: string | string[]
  model?: string
}

export interface EmbeddingResponse {
  data: Array<{
    embedding: number[]
    index: number
  }>
  model: string
  usage: {
    prompt_tokens: number
    total_tokens: number
  }
}

export interface ChatMessage {
  role: "system" | "user" | "assistant"
  content: string
}

export interface ChatCompletionRequest {
  messages: ChatMessage[]
  model?: string
  max_tokens?: number
  temperature?: number
  top_p?: number
  stream?: boolean
  stop?: string[]
}

export interface ChatCompletionResponse {
  id: string
  choices: Array<{
    index: number
    message: {
      role: string
      content: string
    }
    finish_reason: string
  }>
  model: string
  usage: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
}

// ============================================================================
// OPENROUTER CLIENT
// ============================================================================

export class OpenRouterClient {
  private apiKey: string
  private baseUrl: string
  private embeddingModel: string
  private llmModel: string
  private siteUrl?: string
  private siteName?: string
  private verbose: boolean

  constructor(config: OpenRouterConfig) {
    this.apiKey = config.apiKey
    this.baseUrl = config.baseUrl || OPENROUTER_CONFIG.baseUrl
    this.embeddingModel = config.embeddingModel || OPENROUTER_CONFIG.embeddingModel
    this.llmModel = config.llmModel || OPENROUTER_CONFIG.llmModel
    this.siteUrl = config.siteUrl
    this.siteName = config.siteName
    this.verbose = config.verbose || false
  }

  private log(...args: unknown[]): void {
    if (this.verbose) {
      console.log("[OpenRouter]", ...args)
    }
  }

  private getHeaders(): Record<string, string> {
    const headers: Record<string, string> = {
      "Authorization": `Bearer ${this.apiKey}`,
      "Content-Type": "application/json",
      "HTTP-Referer": this.siteUrl || "https://exploit-research.local",
      "X-Title": this.siteName || "Exploit Research Memory System",
    }
    return headers
  }

  /**
   * Generate embeddings using qwen/qwen3-embedding-8b
   */
  async createEmbedding(input: string | string[]): Promise<EmbeddingResponse> {
    const url = `${this.baseUrl}/embeddings`
    const payload = {
      input,
      model: this.embeddingModel,
    }

    this.log(`Creating embedding for ${Array.isArray(input) ? input.length : 1} input(s)`)

    const response = await fetch(url, {
      method: "POST",
      headers: this.getHeaders(),
      body: JSON.stringify(payload),
    })

    if (!response.ok) {
      const error = await response.text()
      throw new Error(`OpenRouter embedding failed: ${response.status} - ${error}`)
    }

    const result = await response.json() as EmbeddingResponse
    this.log(`Embedding created, dimensions: ${result.data[0]?.embedding?.length || 0}`)

    return result
  }

  /**
   * Generate a single embedding vector
   */
  async embed(text: string): Promise<number[]> {
    const response = await this.createEmbedding(text)
    return response.data[0].embedding
  }

  /**
   * Generate multiple embedding vectors
   */
  async embedBatch(texts: string[]): Promise<number[][]> {
    const response = await this.createEmbedding(texts)
    return response.data
      .sort((a, b) => a.index - b.index)
      .map(d => d.embedding)
  }

  /**
   * Chat completion using openai/gpt-5.1
   */
  async createChatCompletion(request: ChatCompletionRequest): Promise<ChatCompletionResponse> {
    const url = `${this.baseUrl}/chat/completions`
    const payload = {
      model: request.model || this.llmModel,
      messages: request.messages,
      max_tokens: request.max_tokens || OPENROUTER_CONFIG.defaultMaxTokens,
      temperature: request.temperature ?? OPENROUTER_CONFIG.defaultTemperature,
      top_p: request.top_p,
      stream: request.stream || false,
      stop: request.stop,
    }

    this.log(`Chat completion with ${request.messages.length} messages`)

    const response = await fetch(url, {
      method: "POST",
      headers: this.getHeaders(),
      body: JSON.stringify(payload),
    })

    if (!response.ok) {
      const error = await response.text()
      throw new Error(`OpenRouter chat completion failed: ${response.status} - ${error}`)
    }

    const result = await response.json() as ChatCompletionResponse
    this.log(`Chat completion finished, tokens: ${result.usage?.total_tokens || 0}`)

    return result
  }

  /**
   * Simple chat helper - send a message and get a response
   */
  async chat(
    userMessage: string,
    options?: {
      systemPrompt?: string
      maxTokens?: number
      temperature?: number
    }
  ): Promise<string> {
    const messages: ChatMessage[] = []

    if (options?.systemPrompt) {
      messages.push({ role: "system", content: options.systemPrompt })
    }

    messages.push({ role: "user", content: userMessage })

    const response = await this.createChatCompletion({
      messages,
      max_tokens: options?.maxTokens,
      temperature: options?.temperature,
    })

    return response.choices[0]?.message?.content || ""
  }

  /**
   * Analyze content using the LLM
   */
  async analyze(
    content: string,
    analysisType: "invariant" | "vulnerability" | "exploit" | "summary" | "custom",
    customPrompt?: string
  ): Promise<string> {
    const prompts: Record<string, string> = {
      invariant: `Analyze the following smart contract code and identify security invariants that should hold true.
For each invariant, provide:
1. The invariant formula or condition
2. Category (access_control, arithmetic, reentrancy, oracle, etc.)
3. Severity (critical, high, medium, low)
4. Confidence level (0-1)

Code to analyze:
${content}`,

      vulnerability: `Analyze the following smart contract code for potential vulnerabilities.
For each vulnerability found, provide:
1. Vulnerability type
2. Location (function/line)
3. Description
4. Severity
5. Potential exploit path

Code to analyze:
${content}`,

      exploit: `Based on the following contract analysis, design a potential exploit path.
Include:
1. Step-by-step attack sequence
2. Required preconditions
3. Estimated profit potential
4. Required capital
5. Complexity rating (1-10)

Analysis:
${content}`,

      summary: `Summarize the following smart contract, highlighting:
1. Main functionality
2. Key state variables
3. External interactions
4. Access control patterns
5. Potential risk areas

Contract:
${content}`,

      custom: customPrompt || content,
    }

    const systemPrompt = `You are an expert smart contract security researcher.
Your task is to analyze DeFi protocols for vulnerabilities and potential exploits.
Be thorough, precise, and focus on actionable findings.
Output should be structured and easy to parse.`

    return this.chat(prompts[analysisType], {
      systemPrompt,
      maxTokens: 8192,
      temperature: 0.3,
    })
  }

  /**
   * Generate Foundry test code for an exploit scenario
   */
  async generateExploitTest(
    scenario: {
      targetContract: string
      vulnerability: string
      attackPath: string[]
      expectedProfit?: number
    }
  ): Promise<string> {
    const prompt = `Generate a Foundry test in Solidity that exploits the following vulnerability:

Target Contract: ${scenario.targetContract}
Vulnerability: ${scenario.vulnerability}
Attack Path:
${scenario.attackPath.map((step, i) => `${i + 1}. ${step}`).join('\n')}
${scenario.expectedProfit ? `Expected Profit: ${scenario.expectedProfit}` : ''}

Requirements:
1. Use Foundry's Test framework
2. Include proper setup with fork configuration
3. Add assertions to verify the exploit succeeded
4. Log key values for debugging
5. Include comments explaining each step

Generate complete, runnable Solidity test code:`

    const systemPrompt = `You are an expert Solidity developer specializing in security testing.
Generate clean, well-commented Foundry test code.
Include all necessary imports and setup.
Use best practices for fork testing.`

    return this.chat(prompt, {
      systemPrompt,
      maxTokens: 8192,
      temperature: 0.2,
    })
  }

  /**
   * Check if similar content exists using embeddings
   */
  async checkSimilarity(
    newContent: string,
    existingEmbeddings: Array<{ id: string; embedding: number[] }>,
    threshold: number = 0.85
  ): Promise<{
    isDuplicate: boolean
    mostSimilarId: string | null
    similarity: number
  }> {
    const newEmbedding = await this.embed(newContent)

    let maxSimilarity = 0
    let mostSimilarId: string | null = null

    for (const existing of existingEmbeddings) {
      const similarity = cosineSimilarity(newEmbedding, existing.embedding)
      if (similarity > maxSimilarity) {
        maxSimilarity = similarity
        mostSimilarId = existing.id
      }
    }

    return {
      isDuplicate: maxSimilarity >= threshold,
      mostSimilarId: maxSimilarity >= threshold ? mostSimilarId : null,
      similarity: maxSimilarity,
    }
  }

  /**
   * Get model info
   */
  getModelInfo() {
    return {
      embeddingModel: this.embeddingModel,
      embeddingDimensions: OPENROUTER_CONFIG.embeddingDimensions,
      llmModel: this.llmModel,
      baseUrl: this.baseUrl,
    }
  }
}

// ============================================================================
// UTILITY FUNCTIONS
// ============================================================================

/**
 * Calculate cosine similarity between two vectors
 */
export function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length !== b.length) {
    throw new Error(`Vector dimension mismatch: ${a.length} vs ${b.length}`)
  }

  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  const magnitude = Math.sqrt(normA) * Math.sqrt(normB)
  if (magnitude === 0) return 0

  return dotProduct / magnitude
}

/**
 * Create an OpenRouter client with environment variables
 */
export function createOpenRouterClient(config?: Partial<OpenRouterConfig>): OpenRouterClient {
  const apiKey = config?.apiKey || process.env.OPENROUTER_API_KEY

  if (!apiKey) {
    throw new Error("OpenRouter API key is required. Set OPENROUTER_API_KEY environment variable or pass apiKey in config.")
  }

  return new OpenRouterClient({
    apiKey,
    ...config,
  })
}

// ============================================================================
// EXPORTS
// ============================================================================

export const openrouter = {
  OpenRouterClient,
  createOpenRouterClient,
  cosineSimilarity,
  OPENROUTER_CONFIG,
} as const
